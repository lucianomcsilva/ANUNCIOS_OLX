{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Trilha 6 - Análise de Sentimentos - Naive Bayes",
      "provenance": [],
      "collapsed_sections": [
        "PkJDxkeJtkqp",
        "GAbQXJsqHo5v",
        "MHHaMIwzHqxL"
      ],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "54999d387da01ac009adc1c3ff01d1799bb96e6f581187d9006fa4c4da7ed267"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucianomcsilva/ANUNCIOS_OLX/blob/main/%5BRIWRS%5D_6_An%C3%A1lise_de_Sentimentos_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ2sbLzPFtR6"
      },
      "source": [
        "# Carregar as bibliotecas e dados"
      ],
      "id": "WQ2sbLzPFtR6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy3kwQPwFtSC"
      },
      "source": [
        "## Bibliotecas"
      ],
      "id": "jy3kwQPwFtSC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkJDxkeJtkqp"
      },
      "source": [
        "### Instalações via PIP"
      ],
      "id": "PkJDxkeJtkqp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaTt7lUTF8-n",
        "outputId": "b543b922-85d2-438a-e0db-d0b34b87b82c"
      },
      "source": [
        "!pip install unidecode\n",
        "!pip install fractions\n",
        "!pip install googletrans\n",
        "!pip install ibm_watson\n",
        "!pip install ibm_cloud_sdk_core\n",
        "!pip install dotenv\n"
      ],
      "id": "OaTt7lUTF8-n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement fractions (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for fractions\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (2021.12.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (1.2.0)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (2022.5.18.1)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ibm_watson in /usr/local/lib/python3.7/dist-packages (6.0.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.27.1)\n",
            "Requirement already satisfied: ibm-cloud-sdk-core==3.*,>=3.3.6 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (3.15.1)\n",
            "Requirement already satisfied: websocket-client==1.1.0 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.8.2)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from ibm-cloud-sdk-core==3.*,>=3.3.6->ibm_watson) (2.4.0)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from ibm-cloud-sdk-core==3.*,>=3.3.6->ibm_watson) (1.26.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.3->ibm_watson) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ibm_cloud_sdk_core in /usr/local/lib/python3.7/dist-packages (3.15.1)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from ibm_cloud_sdk_core) (2.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.26.0 in /usr/local/lib/python3.7/dist-packages (from ibm_cloud_sdk_core) (2.27.1)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from ibm_cloud_sdk_core) (1.26.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from ibm_cloud_sdk_core) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.5.3->ibm_cloud_sdk_core) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.26.0->ibm_cloud_sdk_core) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.26.0->ibm_cloud_sdk_core) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.26.0->ibm_cloud_sdk_core) (2022.5.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dotenv\n",
            "  Using cached dotenv-0.0.5.tar.gz (2.4 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/e2/46/3754073706e31670eed18bfa8a879305b56a471db15f20523c2427b10078/dotenv-0.0.5.tar.gz#sha256=b58d2ab3f83dbd4f8a362b21158a606bee87317a9444485566b3c8f0af847091 (from https://pypi.org/simple/dotenv/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached dotenv-0.0.4.tar.gz (2.0 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/2a/79/933746dc7f4891e5d7de0c113fd3c38cbf76bda0ea1b52df6484e3714928/dotenv-0.0.4.tar.gz#sha256=147fb269f4b65313079c4c2e2c9e74581f1e75c11c92555dcf5b5f48f24599a4 (from https://pypi.org/simple/dotenv/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached dotenv-0.0.2.tar.gz (6.7 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ca/41/d922e0b32beaf7251ff1540a4b5a13c54d6bc25ccd9704294524a1f3af17/dotenv-0.0.2.tar.gz#sha256=6a35c7afd2a1584ccfdcd2af22994b125846ded1fa08ef4a831f4e2281303100 (from https://pypi.org/simple/dotenv/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached dotenv-0.0.1.tar.gz (6.5 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/fa/5a/6dcdddeaa0ddc0bd331fdd1bc8696d9650ede0cb014083716976174eb4b8/dotenv-0.0.1.tar.gz#sha256=04006132a48e301a40b5bc3e8ea0d667a68981f277bb1785af0f8b9f7958e278 (from https://pypi.org/simple/dotenv/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement dotenv (from versions: 0.0.1, 0.0.2, 0.0.4, 0.0.5)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for dotenv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYoTRcF4tp39"
      },
      "source": [
        "### Importações"
      ],
      "id": "UYoTRcF4tp39"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e8c01dc",
        "outputId": "a67abd9e-5171-4cc6-8cac-c32fac55f6c2"
      },
      "source": [
        "# Load EDA Pkgs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load ML Pkgs\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "from textblob import TextBlob\n",
        "\n",
        "#charts & others stuff\n",
        "from unidecode import unidecode\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "from fractions import Fraction\n",
        "import json\n",
        "import os\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "# load_dotenv('../ibm-credentials.env')\n",
        "\n",
        "# IBM Watson\n",
        "from ibm_watson import NaturalLanguageUnderstandingV1\n",
        "from ibm_watson import LanguageTranslatorV3\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "from ibm_watson.natural_language_understanding_v1  import Features, CategoriesOptions, SentimentOptions\n",
        "\n",
        "\n",
        "\n",
        "# NATURAL_LANGUAGE_UNDERSTANDING_APIKEY = os.environ['NATURAL_LANGUAGE_UNDERSTANDING_APIKEY']\n",
        "# NATURAL_LANGUAGE_UNDERSTANDING_URL    = os.environ['NATURAL_LANGUAGE_UNDERSTANDING_URL']\n",
        "\n",
        "# LANGUAGE_TRANSLATOR_APIKEY = os.environ['LANGUAGE_TRANSLATOR_APIKEY']\n",
        "# LANGUAGE_TRANSLATOR_URL    = os.environ['LANGUAGE_TRANSLATOR_URL']"
      ],
      "id": "2e8c01dc",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zpFtedfFtSd"
      },
      "source": [
        "## Carrega o Arquivo"
      ],
      "id": "-zpFtedfFtSd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mNo6sdYFtSe"
      },
      "source": [
        "def RoundSentiment(x):\n",
        "  if x > 0.5:\n",
        "    return 1\n",
        "  elif x < -0.5:\n",
        "    return -1\n",
        "  return 0\n",
        "\n",
        "\n",
        "df_o = pd.read_csv(f\"https://raw.githubusercontent.com/lucianomcsilva/RIWRS/main/tweets_com_ibm_watson_e_ingles.csv\")\n",
        "df_o.drop(df_o.columns[df_o.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "#df[['id_str', 'text', 'screen_name', 'search_term', 'search_term_group', 'source', 'ibm_watson_sentiment', 'ibm_watson_sentiment_label']].sample(10)\n",
        "df = df_o[['id_str', 'text', 'screen_name', 'search_term', 'search_term_group', 'source', 'ibm_watson_sentiment']].loc[df_o['search_term_group'].ne('star+')]\n",
        "df['sentiment'] = df['ibm_watson_sentiment'].apply(lambda x: RoundSentiment(x))"
      ],
      "id": "5mNo6sdYFtSe",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH9COib0Gz_w"
      },
      "source": [
        "# Naive Bayes"
      ],
      "id": "YH9COib0Gz_w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDlvFJQ4v5Jx"
      },
      "source": [
        "## Prepara uma amostra com 100 registros para validarmos a acuracia por nos mesmos"
      ],
      "id": "SDlvFJQ4v5Jx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ri6S58wG-st"
      },
      "source": [
        "# Faz uma amostra apenas com 200 registros aleatorios\n",
        "# Iremos descatar alguns que classificarmos como neutro\n",
        "df_nb = df.loc[df['sentiment'].ne(0)].sample(200)\n",
        "\n",
        "#Salva o arquivo para podermos analisar na mao\n",
        "df_nb.to_csv(\"amostra.csv\")"
      ],
      "id": "2Ri6S58wG-st",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUOMh2bfv5a7"
      },
      "source": [
        "## Carrega o Arquivo Revisado"
      ],
      "id": "QUOMh2bfv5a7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDrDtB8EvxCt"
      },
      "source": [
        "df_nbr = pd.read_csv('amostras-revisado.csv', delimiter=';')\n",
        "df_nbr = df_nbr.loc[df_nbr['sentiment_revised'].ne(0)]\n",
        "#df_nbr_pos = df_nbr.loc[df_nbr['sentiment_revised'].eq(1)]\n"
      ],
      "id": "zDrDtB8EvxCt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nbr"
      ],
      "metadata": {
        "id": "slTPLgwp8ccg"
      },
      "id": "slTPLgwp8ccg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7fYiPwUv4PD"
      },
      "source": [
        ""
      ],
      "id": "d7fYiPwUv4PD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pLauEH0vyrS"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import metrics\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Vectorize the text\n",
        "X = vectorizer.fit_transform(df_nbr['text'])\n",
        "Y = df_nbr['sentiment_revised']\n",
        "\n",
        "# Split dataset\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.33,random_state=42)\n",
        "\n",
        "vocabulary = vectorizer.get_feature_names()\n",
        "dfX = pd.DataFrame(data=X.toarray(), columns=vocabulary) #.iloc[:,0::2]"
      ],
      "id": "3pLauEH0vyrS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJz5bdwpKjqC"
      },
      "source": [
        "# Building Our Model\n",
        "clf = MultinomialNB()\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "print(\"Accuracy of Model :\",clf.score(x_test,y_test))"
      ],
      "id": "PJz5bdwpKjqC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_AtjrQFCuF9"
      },
      "source": [
        "clf.get_params()"
      ],
      "id": "f_AtjrQFCuF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZkZJGN49udP"
      },
      "source": [
        "test_tweet = ['adorei! recomendo titans.', \n",
        "              'Af pessimo',\n",
        "              'netflix'\n",
        "             ]\n",
        "vect = vectorizer.transform(test_tweet).toarray()\n",
        "vect.shape\n",
        "clf.predict(vect)"
      ],
      "id": "JZkZJGN49udP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GemRVDW1GQR"
      },
      "source": [
        "clf.mo"
      ],
      "id": "9GemRVDW1GQR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG0DIKy8_MDt"
      },
      "source": [
        "# class_prob_sorted = clf.feature_log_prob_[0, :].argsort()[::-1]\n",
        "# print(np.take(vectorizer.get_feature_names(), class_prob_sorted[:10]))\n",
        "\n",
        "def get_salient_words(nb_clf, vect, class_ind):\n",
        "    \"\"\"Return salient words for given class\n",
        "    Parameters\n",
        "    ----------\n",
        "    nb_clf : a Naive Bayes classifier (e.g. MultinomialNB, BernoulliNB)\n",
        "    vect : CountVectorizer\n",
        "    class_ind : int\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        a sorted list of (word, log prob) sorted by log probability in descending order.\n",
        "    \"\"\"\n",
        "\n",
        "    words = vect.get_feature_names()\n",
        "    zipped = list(zip(words, nb_clf.feature_log_prob_[class_ind]))\n",
        "    sorted_zip = sorted(zipped, key=lambda t: t[1], reverse=True)\n",
        "\n",
        "    return sorted_zip\n",
        "\n",
        "neg_salient_top_20 = get_salient_words(clf, vectorizer, 0)[:20]\n",
        "pos_salient_top_20 = get_salient_words(clf, vectorizer, 1)[:20]\n",
        "\n"
      ],
      "id": "sG0DIKy8_MDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yswoaslqAt09"
      },
      "source": [
        "neg_salient_top_20"
      ],
      "id": "yswoaslqAt09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_VH_F7TAw3D"
      },
      "source": [
        "pos_salient_top_20"
      ],
      "id": "X_VH_F7TAw3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1QkCHS4FAIF"
      },
      "source": [
        "# NLTK"
      ],
      "id": "j1QkCHS4FAIF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAbQXJsqHo5v"
      },
      "source": [
        "## Simple case"
      ],
      "id": "GAbQXJsqHo5v"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w131ZKaFjR_"
      },
      "source": [
        "def gender_features(word):\n",
        "  return {'last_letter': word[-2:]}"
      ],
      "id": "_w131ZKaFjR_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGaQ_TW-FDIk"
      },
      "source": [
        "from nltk.corpus import names\n",
        "nltk.download('names')\n",
        "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
        "#import random\n",
        "#random.shuffle(labeled_names)"
      ],
      "id": "sGaQ_TW-FDIk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNQHHs-8FS0e"
      },
      "source": [
        "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
        "train_set, test_set = featuresets[500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))\n",
        "classifier.show_most_informative_features(5)"
      ],
      "id": "mNQHHs-8FS0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Cwgft0F0Cf"
      },
      "source": [
        "classifier.classify(gender_features('Andre'))\n",
        "\n"
      ],
      "id": "46Cwgft0F0Cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHHaMIwzHqxL"
      },
      "source": [
        "## Classifier\n"
      ],
      "id": "MHHaMIwzHqxL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIk6xSr7Htxe"
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('movie_reviews')"
      ],
      "id": "aIk6xSr7Htxe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlLn9K-yH0rB"
      },
      "source": [
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "              for category in movie_reviews.categories()\n",
        "              for fileid in movie_reviews.fileids(category)]\n",
        "documents              "
      ],
      "id": "AlLn9K-yH0rB",
      "execution_count": null,
      "outputs": []
    }
  ]
}